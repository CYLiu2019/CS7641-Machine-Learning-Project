---
layout: page
title: Supervised Learning
subtitle: LightGBM, Random Forest, XGBoost
---

<p style="text-align: justify;">
For this project, we attempted a variety of supervised classification methods that are well suited to binary classification problems with severe class imbalance. In this section, we describe some background theory, our approaches and results obtained from this step.
</p>

<p style="text-align: justify;">
  <b>1. Logistic Model</b>
</p>

<p style="text-align: justify;">
  <b>2. Random Forest Classification</b>
</p>
Random forest is a classical tree-based ensemble learning method that constructs multiple individual decision trees in order to evaluate a class assignment of a data point.
<p style="text-align: justify;">
  <b>3. XGBoost Classifier</b>
</p>

<p style="text-align: justify;">
  <b>4. LightGBM Classifier</b>
LightGBM is a gradient boosting tree based supervised learning method. Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees. When we used the LightGBM classifier on our data, we were able to get a score of 0.926 on the kaggle leaderboard. So this algorithm performed well.
</p>
